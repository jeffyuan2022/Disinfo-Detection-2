{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "# train = pd.read_csv('../data/train2.tsv', sep='\\t', header=None)\n",
    "# test = pd.read_csv('../data/test2.tsv', sep='\\t', header=None)\n",
    "data = pd.read_csv('../data/raw/train2.tsv', sep='\\t', header=None)\n",
    "column_names = [\n",
    "    \"Index\",\n",
    "    \"ID\",\n",
    "    \"Label\",\n",
    "    \"Statement\",\n",
    "    \"Subject\",\n",
    "    \"Speaker\",\n",
    "    \"Speaker_Job_Title\",\n",
    "    \"State_Info\",\n",
    "    \"Party_Affiliation\",\n",
    "    \"Barely_True_Counts\",\n",
    "    \"False_Counts\",\n",
    "    \"Half_True_Counts\",\n",
    "    \"Mostly_True_Counts\",\n",
    "    \"Pants_On_Fire_Counts\",\n",
    "    \"Context\",\n",
    "    \"Extracted_Justification\"\n",
    "]\n",
    "\n",
    "# train.columns = column_names\n",
    "# test.columns = column_names\n",
    "data.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clean(df):\n",
    "\n",
    "    # Fill missing values in text-based columns\n",
    "    df.replace({'Statement':''}, np.nan, inplace=True)\n",
    "    df.replace({'Extracted_Justification':''}, np.nan, inplace=True)\n",
    "\n",
    "    # Drop rows with missing 'Statement' or 'Extracted_Justification' (since these are critical)\n",
    "    df.dropna(subset=['Statement', 'Extracted_Justification'], inplace=True)\n",
    "\n",
    "    # Drop rows with missing labels\n",
    "    df = df.dropna(subset=['Label'])\n",
    "\n",
    "    # Impute missing values in categorical columns with 'Unknown'\n",
    "    categorical_columns = ['Speaker', 'Speaker_Job_Title', 'State_Info', 'Party_Affiliation', 'Context']\n",
    "    df[categorical_columns] = df[categorical_columns].fillna('Unknown')\n",
    "\n",
    "    # Impute numerical columns (truth counts) with median values\n",
    "    numeric_columns = [\"Barely_True_Counts\", \"False_Counts\", \"Half_True_Counts\", \"Mostly_True_Counts\", \"Pants_On_Fire_Counts\"]\n",
    "    df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "    df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].median())\n",
    "    return df\n",
    "\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "data = data_clean(data)\n",
    "\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding for categorical variables\n",
    "# {barely-true: 0, false:1, half-true:2, mostly-true:3, pants-fire:4, true:5 }\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "label_encoder.fit(data['Label'])\n",
    "le_name_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "data['Label'] = label_encoder.fit_transform(data['Label']) \n",
    "\n",
    "X = data.drop(['Label'], axis=1)\n",
    "y = data['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained NLP model\n",
    "nltk.download('punkt', quiet=True)  # Tokenizer\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('averaged_perceptron_tagger')  # POS tagger\n",
    "nltk.download('wordnet')  # WordNet for lemmatization\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FF: content statistic\n",
    "\n",
    "def structural_analysis(statement):\n",
    "    doc = nlp(statement)\n",
    "    num_sentences = len(list(doc.sents))\n",
    "    total_tokens = len([token.text for token in doc])\n",
    "    \n",
    "    # Syntactic complexity\n",
    "    avg_sentence_length = total_tokens / num_sentences if num_sentences > 0 else 0\n",
    "    tree_depth = max([token.head.i - token.i for token in doc]) if len(doc) > 0 else 0\n",
    "    \n",
    "    # Sentiment Analysis\n",
    "    sentiment = TextBlob(statement).sentiment\n",
    "    polarity = sentiment.polarity\n",
    "    subjectivity = sentiment.subjectivity\n",
    "\n",
    "    return [avg_sentence_length, tree_depth, polarity, subjectivity]\n",
    "\n",
    "def extract_graph_features(statement):\n",
    "    doc = nlp(statement)\n",
    "    pos_counts = Counter([token.pos_ for token in doc])\n",
    "    entities = Counter([ent.label_ for ent in doc.ents])\n",
    "    \n",
    "    # part of speech tagging\n",
    "    pos_noun = pos_counts.get(\"NOUN\", 0)\n",
    "    pos_verb = pos_counts.get(\"VERB\", 0)\n",
    "    pos_adjective = pos_counts.get(\"ADJ\", 0)\n",
    "    \n",
    "    # named entity recognition\n",
    "    num_persons = entities.get(\"PERSON\", 0)\n",
    "    num_orgs = entities.get(\"ORG\", 0)\n",
    "    num_gpes = entities.get(\"GPE\", 0)\n",
    "    \n",
    "    return [pos_noun, pos_verb, pos_adjective, num_persons, num_orgs, num_gpes]\n",
    "\n",
    "def extract_comparison_features(statement):\n",
    "    # Keywords for different LIWC-like categories (simplified)\n",
    "    cognitive_words = [\"think\", \"know\", \"understand\", \"believe\"]\n",
    "    emotional_words = [\"happy\", \"sad\", \"angry\", \"fear\"]\n",
    "    social_words = [\"friend\", \"family\", \"society\"]\n",
    "\n",
    "    # Tokenize statement and count keywords\n",
    "    vectorizer = CountVectorizer(vocabulary=cognitive_words + emotional_words + social_words)\n",
    "    word_counts = vectorizer.fit_transform([statement]).toarray().flatten()\n",
    "\n",
    "    # Divide word counts into different categories\n",
    "    num_cognitive = sum(word_counts[:len(cognitive_words)])\n",
    "    num_emotional = sum(word_counts[len(cognitive_words):len(cognitive_words) + len(emotional_words)])\n",
    "    num_social = sum(word_counts[-len(social_words):])\n",
    "\n",
    "    return [num_cognitive, num_emotional, num_social]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(statement):\n",
    "    return np.array(structural_analysis(statement) + extract_graph_features(statement) + extract_comparison_features(statement))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape one page of fact-checks\n",
    "def scrape_fact_checks(page_number):\n",
    "    url = f\"https://www.politifact.com/factchecks/?page={page_number}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # List to store the fact-check data\n",
    "    data = []\n",
    "\n",
    "    # Find all fact-check articles on the page\n",
    "    fact_checks = soup.find_all('article', class_='m-statement')\n",
    "\n",
    "    for fact in fact_checks:\n",
    "        # Extract Author/Speaker\n",
    "        author = fact.find('a', class_='m-statement__name').text.strip()\n",
    "\n",
    "        # Extract the Date of the statement\n",
    "        date_string = fact.find('div', class_='m-statement__desc').text.strip()\n",
    "\n",
    "        # Use a regular expression to extract only the date portion (e.g., October 8, 2024)\n",
    "        date_match = re.search(r'([A-Za-z]+ \\d{1,2}, \\d{4})', date_string)\n",
    "        date = date_match.group(0) if date_match else \"No date found\"\n",
    "\n",
    "        # Extract the Claim (statement being fact-checked)\n",
    "        claim = fact.find('div', class_='m-statement__quote').find('a').text.strip()\n",
    "\n",
    "        # Extract the URL to the full fact-check article\n",
    "        link = \"https://www.politifact.com\" + fact.find('div', class_='m-statement__quote').find('a')['href']\n",
    "\n",
    "        # Extract the Rating (e.g., False, Pants on Fire)\n",
    "        rating = fact.find('div', class_='m-statement__meter').find('img')['alt'].strip()\n",
    "\n",
    "        # Append the extracted information to the list\n",
    "        data.append({\n",
    "            'Author/Speaker': author,\n",
    "            'Date': date,\n",
    "            'Claim': claim,\n",
    "            'Rating': rating,\n",
    "            'Link to Full Article': link\n",
    "        })\n",
    "\n",
    "    return data\n",
    "\n",
    "# Loop through multiple pages and collect data\n",
    "def scrape_multiple_pages(start_page, end_page):\n",
    "    all_data = []\n",
    "    for page_number in range(start_page, end_page + 1):\n",
    "        print(f\"Scraping page {page_number}...\")\n",
    "        page_data = scrape_fact_checks(page_number)\n",
    "        all_data.extend(page_data)\n",
    "        time.sleep(2)  # Sleep for 2 seconds between each page request\n",
    "\n",
    "    return all_data\n",
    "\n",
    "# Scrape data from page 1 to 2\n",
    "data = scrape_multiple_pages(1, 2)\n",
    "politifact_data = pd.DataFrame(data)\n",
    "test_link = politifact_data['Link to Full Article'].iloc[0]\n",
    "\n",
    "test_response = requests.get(test_link)\n",
    "soup = BeautifulSoup(test_response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FF: authenticity\n",
    "\n",
    "def cross_referenced(x, politifact_data):\n",
    "    # Get the 'statement' column from the Liar Plus dataset and 'Claim' from PolitiFact\n",
    "    liar_statements = x['Statement']\n",
    "    politifact_claims = politifact_data['Claim']\n",
    "\n",
    "    # Use TF-IDF Vectorizer to convert text to numerical features\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "    # Combine both statements and claims for vectorization\n",
    "    combined_text = pd.concat([liar_statements, politifact_claims], axis=0)\n",
    "\n",
    "    # Fit and transform the combined text using TF-IDF\n",
    "    tfidf_matrix = vectorizer.fit_transform(combined_text)\n",
    "\n",
    "    # Split the transformed matrix into two parts: one for Liar Plus, one for PolitiFact\n",
    "    liar_tfidf = tfidf_matrix[:len(liar_statements)]\n",
    "    politifact_tfidf = tfidf_matrix[len(liar_statements):]\n",
    "\n",
    "    # Compute cosine similarity between every statement in Liar Plus and every claim in PolitiFact\n",
    "    similarity_matrix = cosine_similarity(liar_tfidf, politifact_tfidf)\n",
    "\n",
    "    # Find the highest similarity score for each Liar Plus statement\n",
    "    max_similarity = similarity_matrix.max(axis=1)\n",
    "\n",
    "    # Set a threshold for similarity (e.g., 0.8) to define a \"cross-referenced\" statement\n",
    "    threshold = 0.8\n",
    "    return (max_similarity >= threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a credibility score based on job title (you can customize this based on your data)\n",
    "def assign_credibility_score(job_title):\n",
    "    if \"scientist\" in job_title.lower() or \"doctor\" in job_title.lower():\n",
    "        return 3  # High credibility\n",
    "    elif \"senator\" in job_title.lower() or \"president\" in job_title.lower():\n",
    "        return 2  # Medium credibility\n",
    "    else:\n",
    "        return 1  # Low credibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect if justification contains references to studies or data\n",
    "def contains_cited_data(justification):\n",
    "    keywords = ['according to', 'research', 'study', 'data', 'shown by', 'reported']\n",
    "    for keyword in keywords:\n",
    "        if keyword in justification.lower():\n",
    "            return 1  # Cited data present\n",
    "    return 0  # No cited data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linguistic based, Toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Vectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=50)\n",
    "\n",
    "def fit_vectorizer(X):\n",
    "    \"\"\"Fit the vectorizer on the 'Statement' column of X.\"\"\"\n",
    "    vectorizer.fit(X['Statement'])\n",
    "\n",
    "def transform_data(X):\n",
    "    \"\"\"Transform the data and generate the feature set.\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Token/word count for normalization\n",
    "    X['chunk_length'] = X['Statement'].apply(lambda x: len(x.split()))\n",
    "\n",
    "    # Writing style and linguistic features\n",
    "    X['exclamation_count'] = X['Statement'].apply(lambda x: x.count('!')) / X['chunk_length']\n",
    "    X['question_mark_count'] = X['Statement'].apply(lambda x: x.count('?')) / X['chunk_length']\n",
    "    X['all_caps_count'] = X['Statement'].apply(lambda x: len(re.findall(r'\\b[A-Z]{2,}\\b', x))) / X['chunk_length']\n",
    "    \n",
    "    # Named Entity Count: Counts the number of named entities in the statement using spaCy\n",
    "    X['entity_count'] = X['Statement'].apply(lambda x: len(nlp(x).ents)) / X['chunk_length']\n",
    "    \n",
    "    # Superlative and adjective count using spaCy\n",
    "    X['superlative_count'] = X['Statement'].apply(lambda x: len(re.findall(r'\\b(best|worst|most|least)\\b', x.lower()))) / X['chunk_length']\n",
    "    X['adjective_count'] = X['Statement'].apply(lambda x: len([token for token in nlp(x) if token.pos_ == 'ADJ'])) / X['chunk_length']\n",
    "\n",
    "    # Emotion-based word count\n",
    "    emotion_words = set(['disaster', 'amazing', 'horrible', 'incredible', 'shocking', 'unbelievable'])\n",
    "    X['emotion_word_count'] = X['Statement'].apply(lambda x: len([word for word in x.lower().split() if word in emotion_words])) / X['chunk_length']\n",
    "\n",
    "    # Modal verb count\n",
    "    modal_verbs = set(['might', 'could', 'must', 'should', 'would', 'may'])\n",
    "    X['modal_verb_count'] = X['Statement'].apply(lambda x: len([word for word in x.lower().split() if word in modal_verbs])) / X['chunk_length']\n",
    "\n",
    "    # Complex word ratio\n",
    "    X['complex_word_ratio'] = X['Statement'].apply(lambda x: len([word for word in x.split() if len(re.findall(r'[aeiouy]+', word)) > 2]) / (len(x.split()) + 1))\n",
    "\n",
    "    # Sentiment analysis using VADER\n",
    "    X['sentiment_polarity'] = X['Statement'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "    X['sentiment_subjectivity'] = X['Statement'].apply(lambda x: sid.polarity_scores(x)['neu'])  # Using VADER's neutrality score\n",
    "\n",
    "    # Flesch Reading Ease\n",
    "    X['flesch_reading_ease'] = X['Statement'].apply(flesch_reading_ease)\n",
    "\n",
    "    # Combine all features into a dataframe\n",
    "    numerical_features = X[['exclamation_count', 'question_mark_count', 'all_caps_count',\n",
    "                            'entity_count', 'superlative_count', 'adjective_count',\n",
    "                            'emotion_word_count', 'modal_verb_count', \n",
    "                            'complex_word_ratio', \n",
    "                            'sentiment_polarity', 'sentiment_subjectivity', 'flesch_reading_ease']]\n",
    "    \n",
    "    features.append(numerical_features)\n",
    "    return pd.concat(features, axis=1)\n",
    "\n",
    "def flesch_reading_ease(text):\n",
    "    \"\"\"Compute Flesch Reading Ease score for readability analysis.\"\"\"\n",
    "    sentence_count = max(len(re.split(r'[.!?]+', text)), 1)\n",
    "    word_count = len(text.split())\n",
    "    syllable_count = sum([syllable_count_func(word) for word in text.split()])\n",
    "    if word_count > 0:\n",
    "        return (206.835 - 1.015 * (word_count / sentence_count) - 84.6 * (syllable_count / word_count)) / 100\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def syllable_count_func(word):\n",
    "    \"\"\"Count syllables in a word using regular expressions.\"\"\"\n",
    "    word = word.lower()\n",
    "    syllables = re.findall(r'[aeiouy]+', word)\n",
    "    return max(len(syllables), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the engineered features\n",
    "def combine_feat(X):\n",
    "    X['cross_referenced'] = cross_referenced(X, politifact_data)  # Cross-referencing feature\n",
    "    X['credibility_score'] = X['Speaker_Job_Title'].apply(assign_credibility_score)  # Author credentials feature\n",
    "    X['cited_data'] = X['Extracted_Justification'].apply(contains_cited_data)  # Cited data verification feature\n",
    "    X_auth = X[['cross_referenced', 'credibility_score', 'cited_data']].to_numpy()\n",
    "    X_content = np.vstack(X['Statement'].apply(lambda x: extract_feature(x)).to_numpy())\n",
    "    fit_vectorizer(X)\n",
    "    X_ling_tox = transform_data(X).to_numpy()\n",
    "    X = np.hstack((X_content, X_auth, X_ling_tox))\n",
    "    return X\n",
    "X = combine_feat(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train a Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained model\n",
    "filename = './data/processed/pred_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {barely-true: 0, false:1, half-true:2, mostly-true:3, pants-fire:4, true:5 }\n",
    "# Predictions\n",
    "y_pred = model.predict_proba(X)\n",
    "weights = [0.4, 0.2, 0.6, 0.8, 0, 1]\n",
    "weighted_data = y_pred*weights\n",
    "scores = np.sum(weighted_data, axis=1)\n",
    "len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(result*weights, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
